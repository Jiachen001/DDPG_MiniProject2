{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "train_iter: 0 - update_target_network\n",
      "episode iter: 0 reward: 0.0 number of steps: 180\n",
      "train_iter: 180 epsilon: 0.9999820000000095\n",
      "episode iter: 1 reward: 1.0 number of steps: 260\n",
      "train_iter: 440 epsilon: 0.9999560000000232\n",
      "episode iter: 2 reward: 1.0 number of steps: 233\n",
      "train_iter: 673 epsilon: 0.9999327000000354\n",
      "episode iter: 3 reward: 1.0 number of steps: 244\n",
      "train_iter: 917 epsilon: 0.9999083000000483\n",
      "episode iter: 4 reward: 2.0 number of steps: 281\n",
      "train_iter: 1198 epsilon: 0.9998802000000631\n",
      "episode iter: 5 reward: 0.0 number of steps: 180\n",
      "train_iter: 1378 epsilon: 0.9998622000000725\n",
      "episode iter: 6 reward: 0.0 number of steps: 175\n",
      "train_iter: 1553 epsilon: 0.9998447000000817\n",
      "episode iter: 7 reward: 0.0 number of steps: 172\n",
      "train_iter: 1725 epsilon: 0.9998275000000908\n",
      "episode iter: 8 reward: 3.0 number of steps: 367\n",
      "train_iter: 2092 epsilon: 0.9997908000001101\n",
      "episode iter: 9 reward: 0.0 number of steps: 168\n",
      "train_iter: 2260 epsilon: 0.999774000000119\n",
      "episode iter: 10 reward: 1.0 number of steps: 229\n",
      "train_iter: 2489 epsilon: 0.999751100000131\n",
      "episode iter: 11 reward: 3.0 number of steps: 296\n",
      "train_iter: 2785 epsilon: 0.9997215000001466\n",
      "episode iter: 12 reward: 3.0 number of steps: 349\n",
      "train_iter: 3134 epsilon: 0.999686600000165\n",
      "episode iter: 13 reward: 3.0 number of steps: 319\n",
      "train_iter: 3453 epsilon: 0.9996547000001818\n",
      "episode iter: 14 reward: 1.0 number of steps: 209\n",
      "train_iter: 3662 epsilon: 0.9996338000001928\n",
      "episode iter: 15 reward: 3.0 number of steps: 340\n",
      "train_iter: 4002 epsilon: 0.9995998000002106\n",
      "episode iter: 16 reward: 3.0 number of steps: 364\n",
      "train_iter: 4366 epsilon: 0.9995634000002298\n",
      "episode iter: 17 reward: 0.0 number of steps: 187\n",
      "train_iter: 4553 epsilon: 0.9995447000002396\n",
      "episode iter: 18 reward: 2.0 number of steps: 308\n",
      "train_iter: 4861 epsilon: 0.9995139000002559\n",
      "episode iter: 19 reward: 0.0 number of steps: 190\n",
      "train_iter: 5051 epsilon: 0.9994949000002659\n",
      "episode iter: 20 reward: 1.0 number of steps: 208\n",
      "train_iter: 5259 epsilon: 0.9994741000002768\n",
      "episode iter: 21 reward: 1.0 number of steps: 221\n",
      "train_iter: 5480 epsilon: 0.9994520000002884\n",
      "episode iter: 22 reward: 1.0 number of steps: 231\n",
      "train_iter: 5711 epsilon: 0.9994289000003006\n",
      "episode iter: 23 reward: 0.0 number of steps: 173\n",
      "train_iter: 5884 epsilon: 0.9994116000003097\n",
      "episode iter: 24 reward: 0.0 number of steps: 174\n",
      "train_iter: 6058 epsilon: 0.9993942000003189\n",
      "episode iter: 25 reward: 0.0 number of steps: 199\n",
      "train_iter: 6257 epsilon: 0.9993743000003293\n",
      "episode iter: 26 reward: 0.0 number of steps: 180\n",
      "train_iter: 6437 epsilon: 0.9993563000003388\n",
      "episode iter: 27 reward: 2.0 number of steps: 282\n",
      "train_iter: 6719 epsilon: 0.9993281000003537\n",
      "episode iter: 28 reward: 1.0 number of steps: 259\n",
      "train_iter: 6978 epsilon: 0.9993022000003673\n",
      "episode iter: 29 reward: 1.0 number of steps: 213\n",
      "train_iter: 7191 epsilon: 0.9992809000003785\n",
      "episode iter: 30 reward: 0.0 number of steps: 171\n",
      "train_iter: 7362 epsilon: 0.9992638000003875\n",
      "episode iter: 31 reward: 1.0 number of steps: 241\n",
      "train_iter: 7603 epsilon: 0.9992397000004002\n",
      "episode iter: 32 reward: 0.0 number of steps: 172\n",
      "train_iter: 7775 epsilon: 0.9992225000004092\n",
      "episode iter: 33 reward: 0.0 number of steps: 162\n",
      "train_iter: 7937 epsilon: 0.9992063000004178\n",
      "episode iter: 34 reward: 0.0 number of steps: 181\n",
      "train_iter: 8118 epsilon: 0.9991882000004273\n",
      "episode iter: 35 reward: 1.0 number of steps: 212\n",
      "train_iter: 8330 epsilon: 0.9991670000004385\n",
      "episode iter: 36 reward: 0.0 number of steps: 177\n",
      "train_iter: 8507 epsilon: 0.9991493000004478\n",
      "episode iter: 37 reward: 2.0 number of steps: 273\n",
      "train_iter: 8780 epsilon: 0.9991220000004621\n",
      "episode iter: 38 reward: 3.0 number of steps: 339\n",
      "train_iter: 9119 epsilon: 0.99908810000048\n",
      "episode iter: 39 reward: 0.0 number of steps: 183\n",
      "train_iter: 9302 epsilon: 0.9990698000004896\n",
      "episode iter: 40 reward: 1.0 number of steps: 266\n",
      "train_iter: 9568 epsilon: 0.9990432000005036\n",
      "episode iter: 41 reward: 2.0 number of steps: 304\n",
      "train_iter: 9872 epsilon: 0.9990128000005196\n",
      "train_iter: 10000 - update_target_network\n",
      "episode iter: 42 reward: 1.0 number of steps: 238\n",
      "train_iter: 10110 epsilon: 0.9989890000005321\n",
      "episode iter: 43 reward: 3.0 number of steps: 350\n",
      "train_iter: 10460 epsilon: 0.9989540000005506\n",
      "episode iter: 44 reward: 2.0 number of steps: 256\n",
      "train_iter: 10716 epsilon: 0.998928400000564\n",
      "episode iter: 45 reward: 1.0 number of steps: 221\n",
      "train_iter: 10937 epsilon: 0.9989063000005757\n",
      "episode iter: 46 reward: 1.0 number of steps: 245\n",
      "train_iter: 11182 epsilon: 0.9988818000005886\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-88ddf04901ec>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m                         \u001B[1;31m# pass next states through target network to get q values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m                         \u001B[0mtarget_actions_qvals_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget_actions_qvals\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mtarget_x\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnext_states\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m                         \u001B[0mtarget_max_action_qval_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget_actions_qvals_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m                         \u001B[1;31m# apply Bellman equation to compute target labels: target [for Q(s,a)] = reward + discount * max [over a'] from Q(s', a')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    948\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    949\u001B[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[1;32m--> 950\u001B[1;33m                          run_metadata_ptr)\n\u001B[0m\u001B[0;32m    951\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    952\u001B[0m         \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1171\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1172\u001B[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[1;32m-> 1173\u001B[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001B[0m\u001B[0;32m   1174\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1175\u001B[0m       \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1348\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1349\u001B[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[1;32m-> 1350\u001B[1;33m                            run_metadata)\n\u001B[0m\u001B[0;32m   1351\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1352\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_prun_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1354\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_do_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1355\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1356\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1357\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1358\u001B[0m       \u001B[0mmessage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1339\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1340\u001B[0m       return self._call_tf_sessionrun(\n\u001B[1;32m-> 1341\u001B[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001B[0m\u001B[0;32m   1342\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1343\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_prun_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1427\u001B[0m     return tf_session.TF_SessionRun_wrapper(\n\u001B[0;32m   1428\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_session\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1429\u001B[1;33m         run_metadata)\n\u001B[0m\u001B[0;32m   1430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1431\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_tf_sessionprun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import gym, cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 80\n",
    "K_ACTIONS = 4\n",
    "GAMMA = 0.99\n",
    "TRAIN_EPISODES = 60000\n",
    "TARGET_UPDATE_INTERVAL = 10000\n",
    "SAVE_INTERVAL = 200\n",
    "MAX_BUFFER_SIZE = 500000\n",
    "MIN_BUFFER_SIZE = 50000\n",
    "epsilon, EPSILON_MIN = 1.0, 0.1\n",
    "EPSILON_DELTA = 0.0000001\n",
    "RMS_LR, RMS_DECAY, RMS_MOMENTUM, RMS_EPSILON = 0.0002, 0.99, 0.0, 1e-6\n",
    "\n",
    "def dqn_forward(x):\n",
    "\n",
    "\tx = x/255.0\n",
    "\tx = tf.transpose(x, [0, 2, 3, 1])\n",
    "\t\n",
    "\tx = tf.contrib.layers.conv2d(x, 32, 8, 4)\n",
    "\tx = tf.nn.relu(x)\n",
    "\tx = tf.contrib.layers.conv2d(x, 64, 4, 2)\n",
    "\tx = tf.nn.relu(x)\n",
    "\tx = tf.contrib.layers.conv2d(x, 64, 3, 1)\n",
    "\tx = tf.nn.relu(x)\n",
    "\t\n",
    "\tx = tf.contrib.layers.flatten(x)\n",
    "\tx = tf.contrib.layers.fully_connected(x, 512)\n",
    "\tx = tf.contrib.layers.fully_connected(x, K_ACTIONS)\n",
    "\n",
    "\treturn x\n",
    "\n",
    "def preprocess_image(img):\n",
    "\t\n",
    "\timg = img[30:-15,5:-5:,:]\n",
    "\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\tgray = cv2.resize(gray, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\t\n",
    "\treturn gray\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tenv = gym.envs.make('Breakout-v0')\n",
    "\t\n",
    "\t# agent network\n",
    "\twith tf.variable_scope('agent_network'):\n",
    "\t\tagent_x = tf.placeholder(tf.float32, shape=(None, 4, IMAGE_SIZE, IMAGE_SIZE), name='x')\n",
    "\t\tagent_actions_qvals = dqn_forward(agent_x)\n",
    "\t\n",
    "\t# target network\n",
    "\twith tf.variable_scope('target_network'):\n",
    "\t\ttarget_x = tf.placeholder(tf.float32, shape=(None, 4, IMAGE_SIZE, IMAGE_SIZE))\n",
    "\t\ttarget_actions_qvals = dqn_forward(target_x)\n",
    "\n",
    "\t# compute loss\n",
    "\ttarget_labels = tf.placeholder(tf.float32, shape=(None,))\n",
    "\tbuffer_actions = tf.placeholder(tf.int32, shape=(None,))\n",
    "\tselected_action_values = tf.reduce_max(agent_actions_qvals * tf.one_hot(buffer_actions, K_ACTIONS), axis=1)\n",
    "\tloss = tf.reduce_mean(tf.square(target_labels - selected_action_values))\n",
    "\ttrain_op = tf.train.RMSPropOptimizer(RMS_LR, RMS_DECAY, RMS_MOMENTUM, RMS_EPSILON).minimize(loss)\n",
    "\n",
    "\t# copy weights from agent network to target network\n",
    "\tagent_weights = [t for t in tf.trainable_variables() if 'agent_network' in t.name]\n",
    "\tagent_weights = sorted(agent_weights, key=lambda v: v.name)\n",
    "\ttarget_weights = [t for t in tf.trainable_variables() if 'target_network' in t.name]\n",
    "\ttarget_weights = sorted(target_weights, key=lambda v: v.name)\n",
    "\tupdate_target_network = []\n",
    "\tfor target_weight,agent_weight in zip(target_weights, agent_weights):\n",
    "\t\tupdate_target_network.append(tf.assign(target_weight,agent_weight))\n",
    "\n",
    "\tsess = tf.Session()\n",
    "\tsaver = tf.train.Saver(max_to_keep=1000)\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\t\n",
    "\t# initialize replay buffer\n",
    "\treplay_buffer = []\n",
    "\tobservation = env.reset()\n",
    "\tobservation = preprocess_image(observation)\n",
    "\tstate = np.stack([observation]*4)\n",
    "\tfor i in range(MIN_BUFFER_SIZE):\n",
    "\n",
    "\t\taction = np.random.choice(K_ACTIONS)\n",
    "\t\tobservation, reward, done, _ = env.step(action)\n",
    "\t\tobservation = preprocess_image(observation)\n",
    "\t\tnext_state = np.concatenate([state[1:], np.expand_dims(observation, 0)], axis=0)\n",
    "\t\t\n",
    "\t\treplay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\tstate = next_state\n",
    "\t\telse:\n",
    "\t\t\tobservation = env.reset()\n",
    "\t\t\tobservation = preprocess_image(observation)\n",
    "\t\t\tstate = np.stack([observation]*4)\n",
    "\n",
    "\t# train for number of episodes\n",
    "\ttrain_iter = 0\n",
    "\tfor episode_iter in range(TRAIN_EPISODES):\n",
    "\n",
    "\t\tif (episode_iter + 1)%SAVE_INTERVAL == 0:\n",
    "\t\t\tif not os.path.exists('./models'):\n",
    "\t\t\t\tos.makedirs('./models')\n",
    "\t\t\tsaver.save(sess, './models/net', global_step=episode_iter+1)\n",
    "\t\t\n",
    "\t\t# play single episode, update replay buffer and learn from it\n",
    "\t\tobservation = env.reset()\n",
    "\t\tobservation = preprocess_image(observation)\n",
    "\t\tstate = np.stack([observation]*4)\n",
    "\t\ttotal_reward, num_steps = 0, 0\n",
    "\t\tdone = False\n",
    "\t\twhile not done:\n",
    "\n",
    "\t\t\t# update target network\n",
    "\t\t\tif train_iter % TARGET_UPDATE_INTERVAL == 0:\n",
    "\t\t\t  print ('train_iter:', train_iter, '- update_target_network')\n",
    "\t\t\t  sess.run(update_target_network)\n",
    "\n",
    "\t\t\t# pick an action with respect to epsilon-gredy policy\n",
    "\t\t\tif np.random.random() < epsilon:\n",
    "\t\t\t\taction = np.random.choice(K_ACTIONS)\n",
    "\t\t\telse:\n",
    "\t\t\t\tactions_qvals = sess.run(agent_actions_qvals, feed_dict={agent_x: [state]})[0]\n",
    "\t\t\t\taction = np.argmax(actions_qvals)\n",
    "\t\t\t\t\n",
    "\t\t\tobservation, reward, done, _ = env.step(action)\n",
    "\t\t\tobservation = preprocess_image(observation)\n",
    "\t\t\tnext_state = np.concatenate([state[1:], np.expand_dims(observation, 0)], axis=0)\n",
    "\t\t\t\n",
    "\t\t\t# append sample to replay buffer\n",
    "\t\t\tif len(replay_buffer) == MAX_BUFFER_SIZE: replay_buffer.pop(0)\n",
    "\t\t\treplay_buffer.append((state, action, reward, next_state, done))\n",
    "\t\t\tstate = next_state\n",
    "\t\t\t\n",
    "\t\t\t# sample from replay buffer\n",
    "\t\t\tsamples_batch = random.sample(replay_buffer, BATCH_SIZE) # list of tuples\n",
    "\t\t\tstates, actions, rewards, next_states, dones = zip(*samples_batch) # tuple of lists\n",
    "\n",
    "\t\t\t# pass next states through target network to get q values\n",
    "\t\t\ttarget_actions_qvals_data = sess.run(target_actions_qvals, feed_dict={target_x: next_states})\n",
    "\t\t\ttarget_max_action_qval_data = np.max(target_actions_qvals_data, axis=1)\n",
    "\t\t\t# apply Bellman equation to compute target labels: target [for Q(s,a)] = reward + discount * max [over a'] from Q(s', a')\n",
    "\t\t\ttargets = rewards + np.logical_not(dones).astype(np.float32) * GAMMA * target_max_action_qval_data\n",
    "\n",
    "\t\t\t# update agent network by passing current states and forcing Bellman equation with MSE on sampled actions\n",
    "\t\t\tsess.run(train_op, feed_dict={agent_x: states, target_labels: targets, buffer_actions: actions})\n",
    "\t\t\t\n",
    "\t\t\ttotal_reward = total_reward + reward\n",
    "\t\t\tnum_steps = num_steps + 1\n",
    "\t\t\ttrain_iter = train_iter + 1\n",
    "\t\t\tepsilon = max(epsilon - EPSILON_DELTA, EPSILON_MIN)\n",
    "\n",
    "\t\tprint ('episode iter:', episode_iter, 'reward:', total_reward, 'number of steps:', num_steps)\n",
    "\t\tprint ('train_iter:', train_iter, 'epsilon:', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}